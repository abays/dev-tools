kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: ovn-ovsdb
  namespace: openstack
spec:
  replicas: 3
  serviceName: ovn-ovsdb
  selector:
    matchLabels:
      app: ovn-ovsdb
  template:
    metadata:
      labels:
        app: ovn-ovsdb
        kubernetes.io/os: "linux"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ovn-ovsdb
            topologyKey: kubernetes.io/hostname

      # The only privilege required by this service is to use hostPort, and
      # that's a temporary hack for development use only.
      serviceAccountName: openstack-proto
      containers:

      # nbdb: the northbound, or logical network object DB. In raft mode
      - name: nbdb
        image: &image "quay.io/mbooth/ovn-central:2.11"
        command:
        - /bin/bash
        - -c
        - |
          set -xe -o pipefail

          # See the comment on the override-local-service-ip to see what this
          # is about
          node_svc_name="$(hostname).openstack.svc.cluster.local"

          # The StatefulSet ensures that the first node to come up will be node
          # zero. Below we configure node zero to initialise a new cluster
          # database if none exists already. Other nodes will initialise their
          # database by connecting to node zero. This is only relevant to
          # bootstrapping, and doesn't imply that node zero is the leader.

          initialiser="ovn-ovsdb-0"

          echo "$(date -Iseconds) - starting nbdb"
          if [[ "$(hostname)" == "${initialiser}" ]]; then
            # XXX: Add SSL. service-ca generated certs don't seem to work here.
            #--ovn-nb-db-ssl-key=
            #--ovn-nb-db-ssl-cert=
            #--ovn-nb-db-ssl-ca-cert=
            #--db-nb-cluster-local-proto=ssl
            exec /usr/share/openvswitch/scripts/ovn-ctl \
            --no-monitor \
            --db-nb-create-insecure-remote=yes \
            --db-nb-cluster-local-addr="${node_svc_name}" \
            --db-nb-cluster-local-proto=tcp \
            --ovn-nb-log="-vconsole:${OVN_LOG_LEVEL} -vfile:off" \
            run_nb_ovsdb
          else
            exec /usr/share/openvswitch/scripts/ovn-ctl \
            --no-monitor \
            --db-nb-create-insecure-remote=yes \
            --db-nb-cluster-remote-addr="${initialiser}.openstack.svc.cluster.local" \
            --db-nb-cluster-remote-proto=tcp \
            --db-nb-cluster-local-addr="${node_svc_name}" \
            --db-nb-cluster-local-proto=tcp \
            --ovn-nb-log="-vconsole:${OVN_LOG_LEVEL} -vfile:off" \
            run_nb_ovsdb
          fi
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set -xe
              ovsdb-client wait unix:${OVS_RUNDIR}/ovnnb_db.sock OVN_Northbound connected
        livenessProbe:
          # Try to grab the uuid of the config record by connecting externally
          # to the service IP
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set -xe
              ovsdb-client dump tcp:$(hostname).ovn-ovsdb.openstack.svc.cluster.local:6641 \
                                OVN_Northbound NB_Global _uuid
        env: &pod_env
        - name: OVS_RUNDIR
          value: /pod-run
        - name: OVS_DBDIR
          value: /var/lib/openvswitch
        - name: OVN_LOG_LEVEL
          value: info
        volumeMounts: &pod_mounts
        - mountPath: /var/lib/openvswitch/
          name: data
        - mountPath: /pod-run
          name: pod-run
        - mountPath: /var/log/openvswitch/
          name: logs
        - mountPath: /etc/hosts
          name: hosts
          subPath: hosts
        resources:
          requests:
            cpu: 10m
            memory: 300Mi
        ports:
        - name: nb-db-port
          containerPort: 6641
        - name: nb-db-raft-port
          containerPort: 6643
        terminationMessagePolicy: FallbackToLogsOnError

      # sbdb: The southbound, or flow DB. In raft mode
      - name: sbdb
        image: *image
        command:
        - /bin/bash
        - -c
        - |
          set -xe -o pipefail

          # See the comment on the override-local-service-ip to see what this
          # is about
          node_svc_name="$(hostname).openstack.svc.cluster.local"

          # The StatefulSet ensures that the first node to come up will be node
          # zero. Below we configure node zero to initialise a new cluster
          # database if none exists already. Other nodes will initialise their
          # database by connecting to node zero. This is only relevant to
          # bootstrapping, and doesn't imply that node zero is the leader.

          initialiser="ovn-ovsdb-0"

          echo "$(date -Iseconds) - starting nbdb"
          if [[ "$(hostname)" == "${initialiser}" ]]; then
            # XXX: Add SSL. service-ca generated certs don't seem to work here.
            #--ovn-sb-db-ssl-key=
            #--ovn-sb-db-ssl-cert=
            #--ovn-sb-db-ssl-ca-cert=
            #--db-sb-cluster-local-proto=ssl
            exec /usr/share/openvswitch/scripts/ovn-ctl \
            --no-monitor \
            --db-sb-create-insecure-remote=yes \
            --db-sb-cluster-local-addr="${node_svc_name}" \
            --db-sb-cluster-local-proto=tcp \
            --ovn-sb-log="-vconsole:${OVN_LOG_LEVEL} -vfile:off" \
            run_sb_ovsdb
          else
            exec /usr/share/openvswitch/scripts/ovn-ctl \
            --no-monitor \
            --db-sb-create-insecure-remote=yes \
            --db-sb-cluster-remote-addr="${initialiser}.openstack.svc.cluster.local" \
            --db-sb-cluster-remote-proto=tcp \
            --db-sb-cluster-local-addr="${node_svc_name}" \
            --db-sb-cluster-local-proto=tcp \
            --ovn-sb-log="-vconsole:${OVN_LOG_LEVEL} -vfile:off" \
            run_sb_ovsdb
          fi
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set -xe
              ovsdb-client wait unix:${OVS_RUNDIR}/ovnsb_db.sock OVN_Southbound connected
        livenessProbe:
          # Try to grab the uuid of the config record by connecting externally
          # to the service IP
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set -xe
              ovsdb-client dump tcp:$(hostname).ovn-ovsdb.openstack.svc.cluster.local:6642 \
                                OVN_Southbound SB_Global _uuid
        env: *pod_env
        volumeMounts: *pod_mounts
        ports:
        - name: sb-db-port
          containerPort: 6642
          # XXX: We need to expose the sb db to compute workers, which don't
          # have access to K8S cluster service. By using hostPort here, compute
          # workers can acccess the sb db by lookup up host the pod is running
          # on and accessing it there. This also requires additional privilege.
          # Delete this when we have RHBZ#1836209
          hostPort: 16642
        - name: sb-db-raft-port
          containerPort: 6644
        terminationMessagePolicy: FallbackToLogsOnError

      initContainers:
      # This is a hack because ovsdb-server attempts to open a log file despite
      # it being apparently disabled, and fails if it can't. We should address
      # this with the ovs team and get rid of it.
      - name: create-log-files
        image: *image
        securityContext:
          runAsUser: 0
        command:
        - /bin/bash
        - -c
        - |
          touch /var/log/openvswitch/ovsdb-server-{n,s}b.log
          chmod 777 /var/log/openvswitch/ovsdb-server-{n,s}b.log
        volumeMounts:
        - mountPath: /var/log/openvswitch/
          name: logs


      # ovsdb adds 'cluster-local-addr' to the shared cluster table, and also
      # binds to it locally. Therefore it must be possible for the local pod to
      # bind to this name, and for other pods to route to it.
      #
      # Ideally we would use the pod's IP address from the statefulset service,
      # i.e.:
      #   ovn-ovsdb-0.ovn-ovsdb.openstack.svc.cluster.local
      # however, there are 2 problems which combine to mean we can't do this:
      # * This name is only available from DNS while the pod is running
      # * ovs-dbserver treats DNS names which don't currently resolve as a
      #   syntax error and fails to start.
      # So while a running pod will re-resolve a name which disappears and
      # comes back with a different IP, it won't start at all unless all other
      # pods are running (or all lower ordered pods during initial startup).
      # This means that it's not possible to recover from a situation where 2
      # pods go down.
      #
      # We also create an independent service for each node in the cluster.
      # These services always resolve in DNS whether the pod is up or not, but
      # they're not local addresses, so we can't bind to them.
      #
      # Enter this hack: we add the service name for the local node to
      # /etc/hosts. This means that the local node can bind to it, because it's
      # a local IP address, and other nodes can always resolve the name and
      # connect to it when it's up.
      - name: override-local-service-ip
        image: *image
        securityContext:
          runAsUser: 0
        env:
        - name: POD_IP
          valueFrom:
            fieldRef:
              fieldPath: status.podIP
        command:
          - /bin/bash
          - -c
          - |
            set -e -o pipefail

            cp /etc/hosts /hosts-new
            echo "$POD_IP $(hostname).openstack.svc.cluster.local" >> /hosts-new/hosts
        volumeMounts:
        - mountPath: /hosts-new
          name: hosts

      volumes:
      - name: pod-run
        emptyDir: {}
      - name: logs
        emptyDir: {}
      - name: hosts
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
