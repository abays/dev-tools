kind: StatefulSet
apiVersion: apps/v1
metadata:
  name: ovn-ovsdb
  namespace: openstack
spec:
  replicas: 3
  serviceName: ovn-ovsdb
  selector:
    matchLabels:
      app: ovn-ovsdb
  template:
    metadata:
      labels:
        app: ovn-ovsdb
        kubernetes.io/os: "linux"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ovn-ovsdb
            topologyKey: kubernetes.io/hostname

      # The only privilege required by this service is to use hostPort, and
      # that's a temporary hack for development use only.
      serviceAccountName: openstack-proto
      containers:

      # nbdb: the northbound, or logical network object DB. In raft mode
      - name: nbdb
        image: "tripleotrain/rhel-binary-ovn-nb-db-server:current-tripleo"
        command:
        - /bin/bash
        - -c
        - |
          set -xe

          # The StatefulSet ensures that the first node to come up will be node
          # zero. Below we configure node zero to initialise a new cluster
          # database if none exists already. Other nodes will initialise their
          # database by connecting to node zero. This is only relevant to
          # bootstrapping, and doesn't imply that node zero is the leader.

          initialiser="ovn-ovsdb-0"

          echo "$(date -Iseconds) - starting nbdb"
          if [[ "$(hostname)" == "${initialiser}" ]]; then
            # XXX: Add SSL. service-ca generated certs don't seem to work here.
            #--ovn-nb-db-ssl-key=
            #--ovn-nb-db-ssl-cert=
            #--ovn-nb-db-ssl-ca-cert=
            #--db-nb-cluster-local-proto=ssl
            exec /usr/share/openvswitch/scripts/ovn-ctl \
            --no-monitor \
            --db-nb-create-insecure-remote=yes \
            --db-nb-cluster-local-addr="$(hostname -f)" \
            --db-nb-cluster-local-proto=tcp \
            --ovn-nb-log="-vconsole:${OVN_LOG_LEVEL} -vfile:off" \
            run_nb_ovsdb
          else
            exec /usr/share/openvswitch/scripts/ovn-ctl \
            --no-monitor \
            --db-nb-create-insecure-remote=yes \
            --db-nb-cluster-remote-addr="${initialiser}.ovn-ovsdb.openstack.svc.cluster.local" \
            --db-nb-cluster-remote-proto=tcp \
            --db-nb-cluster-local-addr="$(hostname -f)" \
            --db-nb-cluster-local-proto=tcp \
            --ovn-nb-log="-vconsole:${OVN_LOG_LEVEL} -vfile:off" \
            run_nb_ovsdb
          fi
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set -xe
              ovsdb-client wait unix:${OVS_RUNDIR}/ovnnb_db.sock OVN_Northbound connected
        livenessProbe:
          # Try to grab the uuid of the config record by connecting externally
          # to the service IP
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set -xe
              ovsdb-client dump tcp:$(hostname).ovn-ovsdb.openstack.svc.cluster.local:6641 \
                                OVN_Northbound NB_Global _uuid
        env:
        - name: OVS_RUNDIR
          value: /pod-run
        - name: OVS_DBDIR
          value: /var/lib/openvswitch
        - name: OVN_LOG_LEVEL
          value: info
        volumeMounts:
        - mountPath: /var/lib/openvswitch/
          name: data
        - mountPath: /pod-run
          name: pod-run
        - mountPath: /var/log/openvswitch/
          name: logs
        resources:
          requests:
            cpu: 10m
            memory: 300Mi
        ports:
        - name: nb-db-port
          containerPort: 6641
        - name: nb-db-raft-port
          containerPort: 6643
        terminationMessagePolicy: FallbackToLogsOnError

      # sbdb: The southbound, or flow DB. In raft mode
      - name: sbdb
        image: "tripleotrain/rhel-binary-ovn-nb-db-server:current-tripleo"
        command:
        - /bin/bash
        - -c
        - |
          set -xe

          # The StatefulSet ensures that the first node to come up will be node
          # zero. Below we configure node zero to initialise a new cluster
          # database if none exists already. Other nodes will initialise their
          # database by connecting to node zero. This is only relevant to
          # bootstrapping, and doesn't imply that node zero is the leader.

          initialiser="ovn-ovsdb-0"

          echo "$(date -Iseconds) - starting nbdb"
          if [[ "$(hostname)" == "${initialiser}" ]]; then
            # XXX: Add SSL. service-ca generated certs don't seem to work here.
            #--ovn-sb-db-ssl-key=
            #--ovn-sb-db-ssl-cert=
            #--ovn-sb-db-ssl-ca-cert=
            #--db-sb-cluster-local-proto=ssl
            exec /usr/share/openvswitch/scripts/ovn-ctl \
            --no-monitor \
            --db-sb-create-insecure-remote=yes \
            --db-sb-cluster-local-addr="$(hostname -f)" \
            --db-sb-cluster-local-proto=tcp \
            --ovn-sb-log="-vconsole:${OVN_LOG_LEVEL} -vfile:off" \
            run_sb_ovsdb
          else
            exec /usr/share/openvswitch/scripts/ovn-ctl \
            --no-monitor \
            --db-sb-create-insecure-remote=yes \
            --db-sb-cluster-remote-addr="${initialiser}.ovn-ovsdb.openstack.svc.cluster.local" \
            --db-sb-cluster-remote-proto=tcp \
            --db-sb-cluster-local-addr="$(hostname -f)" \
            --db-sb-cluster-local-proto=tcp \
            --ovn-sb-log="-vconsole:${OVN_LOG_LEVEL} -vfile:off" \
            run_sb_ovsdb
          fi
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set -xe
              ovsdb-client wait unix:${OVS_RUNDIR}/ovnsb_db.sock OVN_Southbound connected
        livenessProbe:
          # Try to grab the uuid of the config record by connecting externally
          # to the service IP
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set -xe
              ovsdb-client dump tcp:$(hostname).ovn-ovsdb.openstack.svc.cluster.local:6642 \
                                OVN_Southbound SB_Global _uuid
        env:
        - name: OVS_RUNDIR
          value: /pod-run
        - name: OVS_DBDIR
          value: /var/lib/openvswitch
        - name: OVN_LOG_LEVEL
          value: info
        volumeMounts:
        - mountPath: /var/lib/openvswitch/
          name: data
        - mountPath: /pod-run
          name: pod-run
        - mountPath: /var/log/openvswitch/
          name: logs
        ports:
        - name: sb-db-port
          containerPort: 6642
          # XXX: We need to expose the sb db to compute workers, which don't
          # have access to K8S cluster service. By using hostPort here, compute
          # workers can acccess the sb db by lookup up host the pod is running
          # on and accessing it there. This also requires additional privilege.
          # Delete this when we have RHBZ#1836209
          hostPort: 16642
        - name: sb-db-raft-port
          containerPort: 6644
        terminationMessagePolicy: FallbackToLogsOnError

      initContainers:
      # This is a hack because ovsdb-server attempts to open a log file despite
      # it being apparently disabled, and fails if it can't. We should address
      # this with the ovs team and get rid of it.
      - name: create-log-files
        image: "tripleotrain/rhel-binary-ovn-nb-db-server:current-tripleo"
        securityContext:
          runAsUser: 0
        command:
        - /bin/bash
        - -c
        - |
          touch /var/log/openvswitch/ovsdb-server-{n,s}b.log
          chmod 777 /var/log/openvswitch/ovsdb-server-{n,s}b.log
        volumeMounts:
        - mountPath: /var/log/openvswitch/
          name: logs

      volumes:
      - name: pod-run
        emptyDir: {}
      - name: logs
        emptyDir: {}
  volumeClaimTemplates:
  - metadata:
      name: data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 1Gi
