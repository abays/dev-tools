---
- hosts: convergence_base
  gather_facts: false
  become: true
  user: root
  vars_files: vars/default.yaml

  tasks:
  - name: Provision storage nodes
    when: ocs_enabled == true
    block:
    - name: Install non-pip dependencies
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - ipmitool
        - virt-install

    - name: Install pip dependencies
      command: "pip3 install {{ item }}"
      with_items:
        - virtualbmc

    - name: Clear host VM memory cache
      shell: echo 3 | tee /proc/sys/vm/drop_caches

    - name: Create storage VMs
      shell: |
        virt-install --ram {{ ocp_storage_memory }} --vcpus {{ ocp_storage_vcpu }} --os-variant rhel8.0 --cpu host-passthrough \
        --disk size={{ ocp_storage_disk }},pool=default,device=disk,bus=virtio,format=qcow2 --import --noautoconsole \
        --vnc --network=bridge:{{ ocp_cluster_name}}pr,mac="52:54:00:82:68:6{{ item }}" \
        --network=bridge:{{ ocp_cluster_name }}bm,mac="52:54:00:82:69:6{{ item }}" --name "{{ ocp_cluster_name}}-storage-{{ item }}" \
        --os-type=linux --events on_reboot=restart --boot hd,network --noreboot
      register: create_vms
      failed_when: create_vms.stderr != "" and "is in use by another virtual machine" not in create_vms.stderr
      with_items:
        - 0
        - 1
        - 2

    - name: Create OCS data disks for VMs
      shell: |
        set -e -o pipefail

        for i in {1..9}; do
            fs="{{ ocs_data_dir }}/ocs_disk_${i}"
            #base="$(echo "{{ ocs_data_dir }}" | rev | cut -d '/' -f2- | rev )"

            if [ ! -f "$fs" ]; then
                # Create a sparse file of the correct size and populate it with an
                # ext2 filesystem.
                mkdir -p {{ ocs_data_dir }}
                truncate -s {{ ocs_disk_size }}G $fs
                mkfs.ext2 -m 0 "$fs"

                # Make world readable
                chown nobody.nobody "$fs"
                chmod 0777 "$fs"
            fi
        done

    - name: Attach disks to storage VMs
      command: "{{ item }}"
      register: attach_disks
      failed_when: attach_disks.stderr != "" and "already exists" not in attach_disks.stderr
      with_items:
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-0 --source {{ ocs_data_dir }}/ocs_disk_1 --target vdb --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-0 --source {{ ocs_data_dir }}/ocs_disk_2 --target vdc --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-0 --source {{ ocs_data_dir }}/ocs_disk_3 --target vdd --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-1 --source {{ ocs_data_dir }}/ocs_disk_4 --target vdb --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-1 --source {{ ocs_data_dir }}/ocs_disk_5 --target vdc --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-1 --source {{ ocs_data_dir }}/ocs_disk_6 --target vdd --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-2 --source {{ ocs_data_dir }}/ocs_disk_7 --target vdb --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-2 --source {{ ocs_data_dir }}/ocs_disk_8 --target vdc --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-2 --source {{ ocs_data_dir }}/ocs_disk_9 --target vdd --persistent" 

    - name: Fixes for dev-scripts vbmc container
      block:
      - name: Stop and remove vbmc container
        shell: podman stop vbmc && podman rm vbmc

      - name: Start vbmc container with proper mounts
        shell: podman run -d --net host --privileged --name vbmc -v "{{ base_path }}/virtualbmc/vbmc":/root/.vbmc -v "/root/.ssh":/root/ssh -v "/var/run/libvirt:/var/run/libvirt:Z" quay.io/metal3-io/vbmc 

    - name: Create and start storage VM vbmcs
      shell: |
        vbmc add "{{ ocp_cluster_name}}-storage-{{ item }}" --address 192.168.111.1 --port "626{{ item }}" --username ADMIN --password ADMIN;
        vbmc start "{{ ocp_cluster_name}}-storage-{{ item }}"
      with_items:
        - 0
        - 1
        - 2

    - name: Open ports for storage VM vbmcs in firewalld
      firewalld:
        port: "626{{ item }}/udp"
        permanent: yes
        state: enabled
        zone: libvirt
        immediate: yes
      with_items:
        - 0
        - 1
        - 2
    
    - name: Shutdown storage VMs using IPMI via vbmcs
      shell: ipmitool -I lanplus -U ADMIN -P ADMIN -H 192.168.111.1 -p "626{{ item }}" power off
      retries: 2
      with_items:
        - 0
        - 1
        - 2

    - name: Prepare DNS/DHCP for storage VMs for dev-scripts environment
      shell: |
        virsh net-update {{ ocp_cluster_name }}bm add ip-dhcp-host "<host mac='52:54:00:82:69:6{{ item }}' name='storage-{{ item }}' ip='192.168.111.3{{ item }}' />" --live --config;
        virsh net-update {{ ocp_cluster_name }}bm add dns-host "<host ip='192.168.111.3{{ item }}'><hostname>storage-{{ item }}</hostname></host>" --live --config
      register: add_dhcp_dns_entries
      failed_when: add_dhcp_dns_entries.stderr != "" and "there is an existing" not in add_dhcp_dns_entries.stderr
      with_items:
        - 0
        - 1
        - 2

- hosts: localhost
  gather_facts: false
  become: true
  user: root
  vars_files: vars/default.yaml
  roles:
  - oc_local
  tasks:
  - name: Install OCS
    when: ocs_enabled == true
    block:
    - name: Get existing {{ ocp_cluster_name }} worker machineset JSON
      shell: "oc get machineset/{{ ocp_cluster_name }}-worker-0 -n openshift-machine-api -o json"
      register: worker_ms_json
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Create OCS YAMLs working dir
      file:
        path: "{{ working_yamls_dir }}/ocs"
        state: directory
        mode: 0755

    - name: Write OCS YAMLs to working dir
      template:
        src: ocs/{{ item }}.yaml.j2
        dest: "{{ working_yamls_dir }}/ocs/{{ item }}.yaml"
      with_items:
        - bmhs
        - local-storage-sub
        - local-storage-volumes
        - machineset
        - ocs-storage-cluster
        - ocs-sub

    - name: Create storage machineset in OCP
      shell: |
        oc apply -f {{ working_yamls_dir }}/ocs/machineset.yaml -n openshift-machine-api
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Create storage baremetal hosts in OCP
      shell: |
        oc apply -f {{ working_yamls_dir }}/ocs/bmhs.yaml -n openshift-machine-api
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for storage baremetal hosts to become ready
      shell: oc get bmh -l test-storage=yes -n openshift-machine-api
      retries: 100
      delay: 30
      register: storage_bmhs_ready
      until: (storage_bmhs_ready.stdout | regex_findall('OK       ready') | length) == 3 or (storage_bmhs_ready.stdout | regex_findall('OK       provisioned') | length) == 3
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Scale storage machineset to 3
      shell: oc scale machineset/{{ ocp_cluster_name }}-storage-0 --replicas=3 -n openshift-machine-api
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for storage nodes to be provisioned
      shell: oc get nodes -l node-role.kubernetes.io/storage
      retries: 100
      delay: 30
      register: storage_nodes_ready
      until: (storage_nodes_ready.stdout | regex_findall('Ready    storage') | length) == 3
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Label storage nodes as local-storage capable
      shell: |
        for i in $(oc get nodes --no-headers -l node-role.kubernetes.io/storage -o name); do
          oc label $i cluster.ocs.openshift.io/openshift-storage=''
        done
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"
    
    - name: Deploy local-storage subscription
      shell: |
        oc new-project local-storage
        oc annotate project local-storage --overwrite openshift.io/node-selector=''
        oc apply -f {{ working_yamls_dir }}/ocs/local-storage-sub.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for local-storage operator to be ready
      shell: oc get pods -n local-storage
      retries: 100
      delay: 20
      register: local_storage_operator_ready
      until: (local_storage_operator_ready.stdout | regex_findall('local-storage-operator-.+-.+Running') | length) == 1
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Create local-storage PVs
      shell: |
        oc apply -f {{ working_yamls_dir }}/ocs/local-storage-volumes.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for local-storage PVs to be ready
      shell: oc get pv -n local-storage
      retries: 100
      delay: 20
      register: local_storage_volumes_ready
      until: (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+Available') | length) == (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+') | length) and (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+Available') | length) != 0
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Deploy OCS subscription
      shell: oc apply -f {{ working_yamls_dir }}/ocs/ocs-sub.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for OCS, rook-ceph and noobaa operators to be ready
      shell: oc get pods -n openshift-storage
      retries: 100
      delay: 20
      register: ocs_operators_ready
      until: (ocs_operators_ready.stdout | regex_findall('ocs-operator-.+-.+Running') | length) == 1 and (ocs_operators_ready.stdout | regex_findall('rook-ceph-operator-.+-.+Running') | length) == 1 and (ocs_operators_ready.stdout | regex_findall('noobaa-operator-.+-.+Running') | length) == 1
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Deploy OCS cluster
      shell: oc apply -f {{ working_yamls_dir }}/ocs/ocs-storage-cluster.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for OCS pods to be ready
      shell: oc get pods -n openshift-storage
      retries: 100
      delay: 30
      register: ocs_pods_ready
      until: ((ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+') | length) and (ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+') | length) and (ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+') | length) and (ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+') | length) and (ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('noobaa-core-0.+Running') | length) == 1)
             and ((ocs_pods_ready.stdout | regex_findall('noobaa-db-0.+Running') | length) == 1)
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"
