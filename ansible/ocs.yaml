---
- hosts: convergence_base
  gather_facts: false
  become: true
  user: root
  vars_files: vars/default.yaml
  roles:
  - oc_local

  tasks:
  - name: Provision storage nodes and install OCS
    when: ocs_enabled == true
    block:
    - name: Install non-pip dependencies
      package:
        name: "{{ item }}"
        state: latest
      with_items:
        - ipmitool
        - virt-install

    - name: Install pip dependencies
      command: "pip3 install {{ item }}"
      with_items:
        - virtualbmc

    - name: Clear host VM memory cache
      shell: echo 3 | tee /proc/sys/vm/drop_caches

    - name: Create storage VMs
      shell: |
        virt-install --ram 32768 --vcpus 12 --os-variant rhel8.0 --cpu host-passthrough \
        --disk size={{ ocp_worker_disk }},pool=default,device=disk,bus=virtio,format=qcow2 --import --noautoconsole \
        --vnc --network=bridge:{{ ocp_cluster_name}}pr,mac="52:54:00:82:68:6{{ item }}" \
        --network=bridge:{{ ocp_cluster_name }}bm,mac="52:54:00:82:69:6{{ item }}" --name "{{ ocp_cluster_name}}-storage-{{ item }}" \
        --os-type=linux --events on_reboot=restart --boot hd,network --noreboot
      register: create_vms
      failed_when: create_vms.stderr != "" and "is in use by another virtual machine" not in create_vms.stderr
      with_items:
        - 0
        - 1
        - 2

    - name: Read host {{ ocs_device }} information
      parted: device={{ ocs_device }} unit=GiB
      register: device_info

    - debug:
        var: device_info

    - name: Prepare partitions on {{ ocs_device }}
      when: (device_info.partitions | length) == 0
      block:
        - name: Set msdos disk extended partition range end
          set_fact:
            extended_part_end: "100%"
          when: device_info.disk.table == "msdos"

        - name: Set non-msdos disk extended partition range end
          set_fact:
            extended_part_end: "40%"
          when: device_info.disk.table != "msdos"

        - name: Create partitions on {{ ocs_device }}
          shell: |
            parted {{ ocs_device }} mkpart primary ext2 0% 10%;
            parted {{ ocs_device }} mkpart primary ext2 10% 20%;
            parted {{ ocs_device }} mkpart primary ext2 20% 30%;
            parted {{ ocs_device }} mkpart extended 30% {{ extended_part_end }};
            parted {{ ocs_device }} mkpart logical ext2 40% 50%;
            parted {{ ocs_device }} mkpart logical ext2 50% 60%;
            parted {{ ocs_device }} mkpart logical ext2 60% 70%;
            parted {{ ocs_device }} mkpart logical ext2 70% 80%;
            parted {{ ocs_device }} mkpart logical ext2 80% 90%;
            parted {{ ocs_device }} mkpart logical ext2 90% 100%

    - name: Re-read host {{ ocs_device }} information
      parted: device={{ ocs_device }} unit=GiB
      register: device_info

    - debug:
        var: device_info

    - name: Attach disks to storage VMs
      command: "{{ item }}"
      register: attach_disks
      failed_when: attach_disks.stderr != "" and "already exists" not in attach_disks.stderr
      with_items:
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-0 --source {{ ocs_device }}1 --target vdb --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-0 --source {{ ocs_device }}2 --target vdc --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-0 --source {{ ocs_device }}3 --target vdd --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-1 --source {{ ocs_device }}5 --target vdb --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-1 --source {{ ocs_device }}6 --target vdc --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-1 --source {{ ocs_device }}7 --target vdd --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-2 --source {{ ocs_device }}8 --target vdb --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-2 --source {{ ocs_device }}9 --target vdc --persistent"
        - "virsh attach-disk {{ ocp_cluster_name }}-storage-2 --source {{ ocs_device }}10 --target vdd --persistent"    

    - name: Fixes for dev-scripts vbmc container
      when: not use_kni_ipi_virt
      block:
      - name: Stop and remove vbmc container
        shell: podman stop vbmc && podman rm vbmc

      - name: Start vbmc container with proper mounts
        shell: podman run -d --net host --privileged --name vbmc -v "{{ base_path }}/virtualbmc/vbmc":/root/.vbmc -v "/root/.ssh":/root/ssh -v "/var/run/libvirt:/var/run/libvirt:Z" quay.io/metal3-io/vbmc 

    - name: Create and start storage VM vbmcs
      shell: |
        vbmc add "{{ ocp_cluster_name}}-storage-{{ item }}" --address 192.168.111.1 --port "626{{ item }}" --username ADMIN --password ADMIN;
        vbmc start "{{ ocp_cluster_name}}-storage-{{ item }}"
      with_items:
        - 0
        - 1
        - 2

    - name: Open ports for storage VM vbmcs in firewalld
      firewalld:
        port: "626{{ item }}/udp"
        permanent: yes
        state: enabled
        zone: "{{ (use_kni_ipi_virt) | ternary('public','libvirt') }}"
        immediate: yes
      with_items:
        - 0
        - 1
        - 2
    
    - name: Shutdown storage VMs using IPMI via vbmcs
      shell: ipmitool -I lanplus -U ADMIN -P ADMIN -H 192.168.111.1 -p "626{{ item }}" power off
      retries: 2
      with_items:
        - 0
        - 1
        - 2

    - name: Prepare DNS/DHCP for storage VMs for dev-scripts environment
      when: not use_kni_ipi_virt
      shell: |
        virsh net-update {{ ocp_cluster_name }}bm add ip-dhcp-host "<host mac='52:54:00:82:69:6{{ item }}' name='storage-{{ item }}' ip='192.168.111.3{{ item }}' />" --live --config;
        virsh net-update {{ ocp_cluster_name }}bm add dns-host "<host ip='192.168.111.3{{ item }}'><hostname>storage-{{ item }}</hostname></host>" --live --config
      register: add_dhcp_dns_entries
      failed_when: add_dhcp_dns_entries.stderr != "" and "there is an existing" not in add_dhcp_dns_entries.stderr
      with_items:
        - 0
        - 1
        - 2

    - name: Prepare DNS/DHCP for storage VMs for kni-ipi-virt environment
      when: use_kni_ipi_virt
      block:
      - name: Add storage VM hostnames and IPs to DHCP server
        blockinfile:
          path: "{{ base_path }}/dev-scripts/dhcp/generated/bm/etc/dnsmasq.d/dnsmasq.hostsfile"
          block: |
            52:54:00:82:69:60,192.168.111.140,{{ ocp_cluster_name }}-storage-0.{{ ocp_cluster_name }}.test.metalkube.org
            52:54:00:82:69:61,192.168.111.141,{{ ocp_cluster_name }}-storage-1.{{ ocp_cluster_name }}.test.metalkube.org
            52:54:00:82:69:62,192.168.111.142,{{ ocp_cluster_name }}-storage-2.{{ ocp_cluster_name }}.test.metalkube.org
      
      - name: Restart DHCP server
        shell: |
          podman stop ipi-dnsmasq-bm;
          podman start ipi-dnsmasq-bm

      - name: Add storage VM hostnames and IP octet to DNS server
        blockinfile:
          path: "{{ base_path }}/dev-scripts/dns/generated/db.reverse"
          block: |
            140 IN  PTR {{ ocp_cluster_name }}-storage-0.{{ ocp_cluster_name }}.test.metalkube.org.
            141 IN  PTR {{ ocp_cluster_name }}-storage-1.{{ ocp_cluster_name }}.test.metalkube.org.
            142 IN  PTR {{ ocp_cluster_name }}-storage-2.{{ ocp_cluster_name }}.test.metalkube.org.

      - name: Add storage VM hostnames and IPs DNS server
        blockinfile:
          path: "{{ base_path }}/dev-scripts/dns/generated/db.zone"
          block: |
            {{ ocp_cluster_name }}-storage-0                          A 192.168.111.140
            {{ ocp_cluster_name }}-storage-1                          A 192.168.111.141
            {{ ocp_cluster_name }}-storage-2                          A 192.168.111.142

      - name: Restart DNS server
        shell: |
          podman stop ipi-coredns;
          podman start ipi-coredns
        become: yes
        become_user: ocp

    # - name: Get RHCOS image URL used for workers
    #   shell: "oc get machineset/{{ ocp_cluster_name }}-worker-0 -n openshift-machine-api -o json | jq -r '.spec.template.spec.providerSpec.value.image.url'"
    #   register: rhcos_image_url
    #   environment:
    #     PATH: "{{ oc_env_path }}"
    #     KUBECONFIG: "{{ kubeconfig }}"

    # - name: Get RHCOS image checksum used for workers
    #   shell: "oc get machineset/{{ ocp_cluster_name }}-worker-0 -n openshift-machine-api -o json | jq -r '.spec.template.spec.providerSpec.value.image.checksum'"
    #   register: rhcos_image_checksum
    #   environment:
    #     PATH: "{{ oc_env_path }}"
    #     KUBECONFIG: "{{ kubeconfig }}"

    - name: Get existing {{ ocp_cluster_name }} worker machineset JSON
      shell: "oc get machineset/{{ ocp_cluster_name }}-worker-0 -n openshift-machine-api -o json"
      register: worker_ms_json
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Create OCS YAMLs working dir
      file:
        path: "{{ working_yamls_dir }}/ocs"
        state: directory
        mode: 0755

    - name: Write OCS YAMLs to working dir
      template:
        src: ocs/{{ item }}.yaml.j2
        dest: "{{ working_yamls_dir }}/ocs/{{ item }}.yaml"
      with_items:
        - bmhs
        - local-storage-sub
        - local-storage-volumes
        - machineset
        - ocs-storage-cluster
        - ocs-sub

    - name: Create storage machineset in OCP
      shell: |
        oc apply -f {{ working_yamls_dir }}/ocs/machineset.yaml -n openshift-machine-api
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Create storage baremetal hosts in OCP
      shell: |
        oc apply -f {{ working_yamls_dir }}/ocs/bmhs.yaml -n openshift-machine-api
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for storage baremetal hosts to become ready
      shell: oc get bmh -l test-storage=yes -n openshift-machine-api
      retries: 100
      delay: 30
      register: storage_bmhs_ready
      until: (storage_bmhs_ready.stdout | regex_findall('OK       ready') | length) == 3 or (storage_bmhs_ready.stdout | regex_findall('OK       provisioned') | length) == 3
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Scale storage machineset to 3
      shell: oc scale machineset/{{ ocp_cluster_name }}-storage-0 --replicas=3 -n openshift-machine-api
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for storage nodes to be provisioned
      shell: oc get nodes -l node-role.kubernetes.io/storage
      retries: 100
      delay: 30
      register: storage_nodes_ready
      until: (storage_nodes_ready.stdout | regex_findall('Ready    storage') | length) == 3
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Label storage nodes as local-storage capable
      shell: |
        for i in $(oc get nodes --no-headers -l node-role.kubernetes.io/storage -o name); do
          oc label $i cluster.ocs.openshift.io/openshift-storage=''
        done
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"
    
    - name: Deploy local-storage subscription
      shell: |
        oc new-project local-storage
        oc annotate project local-storage --overwrite openshift.io/node-selector=''
        oc apply -f {{ working_yamls_dir }}/ocs/local-storage-sub.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for local-storage operator to be ready
      shell: oc get pods -n local-storage
      retries: 100
      delay: 20
      register: local_storage_operator_ready
      until: (local_storage_operator_ready.stdout | regex_findall('local-storage-operator-.+-.+Running') | length) == 1
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Create local-storage PVs
      shell: |
        oc apply -f {{ working_yamls_dir }}/ocs/local-storage-volumes.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for local-storage PVs to be ready
      shell: oc get pv -n local-storage
      retries: 100
      delay: 20
      register: local_storage_volumes_ready
      until: (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+Available') | length) == (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+') | length) and (local_storage_volumes_ready.stdout | regex_findall('local-pv-.+Available') | length) != 0
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Deploy OCS subscription
      shell: oc apply -f {{ working_yamls_dir }}/ocs/ocs-sub.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for OCS, rook-ceph and noobaa operators to be ready
      shell: oc get pods -n openshift-storage
      retries: 100
      delay: 20
      register: ocs_operators_ready
      until: (ocs_operators_ready.stdout | regex_findall('ocs-operator-.+-.+Running') | length) == 1 and (ocs_operators_ready.stdout | regex_findall('rook-ceph-operator-.+-.+Running') | length) == 1 and (ocs_operators_ready.stdout | regex_findall('noobaa-operator-.+-.+Running') | length) == 1
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Deploy OCS cluster
      shell: oc apply -f {{ working_yamls_dir }}/ocs/ocs-storage-cluster.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for OCS pods to be ready
      shell: oc get pods -n openshift-storage
      retries: 100
      delay: 30
      register: ocs_pods_ready
      until: ((ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+') | length) and (ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+') | length) and (ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+') | length) and (ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+') | length) and (ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('noobaa-core-0.+Running') | length) == 1)
             and ((ocs_pods_ready.stdout | regex_findall('noobaa-db-0.+Running') | length) == 1)
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"
