---
- hosts: convergence_base
  gather_facts: false
  become: true
  user: root
  vars_files: vars/default.yaml

  tasks:
  - name: Prepare {{ ocs_worker_domain }} VM for use as a storage node
    when: ocs_enabled == true
    block:
    - name: Clear host VM memory cache
      shell: echo 3 | tee /proc/sys/vm/drop_caches

    - name: Create OCS data disks for {{ ocs_worker_domain }} VM
      shell: |
        set -e -o pipefail

        for i in {1..{{ ocs_disks | length }}}; do
            fs="{{ ocs_data_dir }}/ocs_disk_${i}"

            if [ ! -f "$fs" ]; then
                # Create a sparse file of the correct size and populate it with an
                # ext2 filesystem.
                mkdir -p {{ ocs_data_dir }}
                truncate -s {{ ocs_disk_size }}G $fs
                mkfs.ext2 -m 0 "$fs"

                # Make world readable
                chown nobody.nobody "$fs"
                chmod 0777 "$fs"
            fi
        done

    - name: Stop {{ ocs_worker_domain }} VM
      virt: 
        name: "{{ ocs_worker_domain }}"
        state: destroyed

    - name: Attach data disks to {{ ocs_worker_domain }} VM
      command: "virsh attach-disk {{ ocs_worker_domain }} --source {{ ocs_data_dir }}/ocs_disk_{{ index + 1 }} --target {{ item }} --persistent"
      register: attach_disks
      failed_when: attach_disks.stderr != "" and "already exists" not in attach_disks.stderr
      loop: "{{ ocs_disks }}"
      loop_control:
        index_var: index

    - name: Set {{ ocs_worker_domain }} VM cpu and memory specs
      block:
      - name: Set {{ ocs_worker_domain }} VM memory
        shell: |
          virsh setmaxmem {{ ocs_worker_domain }} {{ ocp_storage_memory }}M --config;
          virsh setmem {{ ocs_worker_domain }} {{ ocp_storage_memory }}M --config

      - name: Set {{ ocs_worker_domain }} VM max cpus
        command: "virsh setvcpus {{ ocs_worker_domain }} {{ ocp_storage_vcpu }} --config --maximum"

      - name: Restart {{ ocs_worker_domain }} VM
        virt: 
          name: "{{ ocs_worker_domain }}"
          state: running

      - name: Set {{ ocs_worker_domain }} VM live cpus
        command: "virsh setvcpus {{ ocs_worker_domain }} {{ ocp_storage_vcpu }} --live"

- hosts: localhost
  gather_facts: false
  become: true
  user: root
  vars_files: vars/default.yaml
  roles:
  - oc_local
  tasks:
  - name: Install OCS
    when: ocs_enabled == true
    block:
    - name: Create OCS YAMLs working dir
      file:
        path: "{{ working_yamls_dir }}/ocs"
        state: directory
        mode: 0755

    - name: Write OCS YAMLs to working dir
      template:
        src: ocs/{{ item }}.yaml.j2
        dest: "{{ working_yamls_dir }}/ocs/{{ item }}.yaml"
      with_items:
        - local-storage-sub
        - local-storage-volumes
        - ocs-storage-cluster
        - ocs-sub

    - name: Label {{ ocs_worker_node }} as OCS-capable
      shell: |
          oc label node/{{ ocs_worker_node }} cluster.ocs.openshift.io/openshift-storage='' --overwrite=true;
          oc label node/{{ ocs_worker_node }} topology.rook.io/rack=rack0 --overwrite=true
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"
    
    - name: Deploy local-storage subscription
      shell: |
        oc new-project local-storage
        oc annotate project local-storage --overwrite openshift.io/node-selector=''
        oc apply -f {{ working_yamls_dir }}/ocs/local-storage-sub.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for local-storage operator to be ready
      shell: oc get pods -n local-storage
      retries: 100
      delay: 20
      register: local_storage_operator_ready
      until: (local_storage_operator_ready.stdout | regex_findall('local-storage-operator-.+-.+Running') | length) == 1
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Create local-storage PVs
      shell: |
        oc apply -f {{ working_yamls_dir }}/ocs/local-storage-volumes.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Deploy OCS subscription
      shell: oc apply -f {{ working_yamls_dir }}/ocs/ocs-sub.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for OCS, rook-ceph and noobaa operators to be ready
      shell: oc get pods -n openshift-storage
      retries: 100
      delay: 20
      register: ocs_operators_ready
      until: (ocs_operators_ready.stdout | regex_findall('ocs-operator-.+-.+Running') | length) == 1 and (ocs_operators_ready.stdout | regex_findall('rook-ceph-operator-.+-.+Running') | length) == 1 and (ocs_operators_ready.stdout | regex_findall('noobaa-operator-.+-.+Running') | length) == 1
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Deploy OCS cluster
      shell: oc apply -f {{ working_yamls_dir }}/ocs/ocs-storage-cluster.yaml
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Wait for OCS pods to be ready
      shell: oc get pods -n openshift-storage
      retries: 100
      delay: 30
      register: ocs_pods_ready
      until: ((ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+') | length) and (ocs_pods_ready.stdout | regex_findall('csi-cephfsplugin-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+') | length) and (ocs_pods_ready.stdout | regex_findall('csi-rbdplugin-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+') | length) and (ocs_pods_ready.stdout | regex_findall('rook-ceph-mon-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+Running') | length) == (ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+') | length) and (ocs_pods_ready.stdout | regex_findall('rook-ceph-osd-\d-.+Running') | length) != 0)
             and ((ocs_pods_ready.stdout | regex_findall('noobaa-core-0.+Running') | length) == 1)
             and ((ocs_pods_ready.stdout | regex_findall('noobaa-db-0.+Running') | length) == 1)
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - name: Enable Ceph tools pod
      shell: "oc patch OCSInitialization ocsinit -n openshift-storage --type json --patch '[{ \"op\": \"replace\", \"path\": \"/spec/enableCephTools\", \"value\": true }]'"
      environment:
        PATH: "{{ oc_env_path }}"
        KUBECONFIG: "{{ kubeconfig }}"

    - debug:
        msg:
          - "You can now check the health of your OCS cluster via:"
          - "oc rsh -n openshift-storage $(oc get pods -n openshift-storage -l app=rook-ceph-tools -o name)"
          - "ceph -s"
