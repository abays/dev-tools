---
- hosts: localhost
  vars_files: vars/default.yaml
  roles:
  - oc_local

  tasks:
  - name: Create yaml dir
    import_role: name=working_yaml_dir
    vars:
      name: compute_node

  - name: Copy files to yaml dir
    copy:
      src: "{{ item }}"
      dest: "{{ compute_node_yaml_dir }}"
    with_fileglob:
    - "ocp/compute-node/*"

  - name: install compute CR and openstack client config
    shell: |
      oc apply -f "{{ compute_node_yaml_dir }}"
    environment: &oc_env
      PATH: "{{ oc_env_path }}"
      KUBECONFIG: "{{ kubeconfig }}"

  - name: set deletePolicy=Newest on existing worker machineset for predictable scale down
    shell: |
      set -e -o pipefail

      oc get machineset/ostest-worker-0 -n openshift-machine-api -o json | \
          jq '.spec.deletePolicy="Newest"' | oc apply -f -
    environment:
      <<: *oc_env

  - name: get a list of all worker baremetalhosts
    shell: |
      set -e -o pipefail

      oc get -n openshift-machine-api bmh --no-headers -o custom-columns=name:.metadata.name | \
          grep "worker-"
    environment:
      <<: *oc_env
    register: baremetalhosts_out

  - set_fact:
      baremetalhosts: "{{ baremetalhosts_out.stdout_lines }}"

  - name: label all worker baremetalhosts as potential ocp worker nodes
    shell: |
      oc patch -n openshift-machine-api -p '{"metadata":{"labels": { "ospRole": "worker-osp" }}}' \
          --type merge bmh/{{ item }}
    environment:
      <<: *oc_env
    loop: "{{ baremetalhosts }}"

  # XXX(mdbooth): This will fail if workers is set to zero, because this
  # results in the value being removed. I explicitly set 'jq -e' to error in
  # this case, so we could just remove it, but I added it because we were
  # missing real errorsit in other places. For now, just don't set it to zero, I guess?
  - name: Get the target number of osp workers
    shell: |
      set -e -o pipefail

      oc get -n openstack computenodeopenstack/worker-osp -o json | jq -re '.spec.workers'
    environment:
      <<: *oc_env
    register: workers_out

  - set_fact:
      target_workers: "{{ workers_out.stdout }}"

  - name: scale worker machineset to {{ worker_0_scale }}
    shell: |
      oc -n openshift-machine-api scale machineset {{ ocp_cluster_name }}-worker-0 \
          --replicas={{ worker_0_scale }}
    environment:
      <<: *oc_env
    vars:
      worker_0_scale: "{{ baremetalhosts|length - target_workers|int }}"

  - name: wait until worker-osp got provisioned
    shell: |
      set -e -o pipefail

      oc get -n openshift-machine-api machineset/{{ ocp_cluster_name }}-worker-osp -o json | jq -re '.status.readyReplicas'
    environment:
      <<: *oc_env
    register: worker_osp_ready
    until: worker_osp_ready is not failed and worker_osp_ready.stdout == target_workers
    delay: 30
    retries: 60
    tags:
    - wait
