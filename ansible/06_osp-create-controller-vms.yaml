---
- hosts: convergence_base
  become: true
  become_user: root
  tasks:
  - name: Include variables
    include_vars: vars/default.yaml

  - name: create worker-osp-controller dir
    file:
      path: "{{ base_path }}/worker-osp-controller"
      state: directory

  - name: get rhcos image url from existing machineset
    become_user: ocp
    shell: >
      oc get -n openshift-machine-api machineset/{{ ocp_cluster_name }}-worker-0 -o json | jq -r .spec.template.spec.providerSpec.value.image.url
    environment:
      PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
      KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
    register: rhcos_image_url_ret

  - name: create worker-osp-controller-machineset.yaml from template
    vars:
      rhcos_image_url: "{{ rhcos_image_url_ret.stdout }}"
    template:
      src: "worker-osp-controller-machineset.yaml.j2"
      dest: "{{ base_path }}/worker-osp-controller/worker-osp-controller-machineset.yaml"
      mode: 0644

  - name: oc apply -f {{ base_path }}/worker-osp-controller/worker-osp-controller-machineset.yaml
    become_user: ocp
    shell: >
      oc apply -f {{ base_path }}/worker-osp-controller/worker-osp-controller-machineset.yaml
    environment:
      PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
      KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"

  - name: scale OSP controller worker
    block:
    - name: annotate workers for deletion to scale default workers down
      become_user: ocp
      shell: |
        worker=$(oc get node/worker-{{ item }} -o json | jq -r '.metadata.annotations["machine.openshift.io/machine"]' | cut -d/ -f2)
        oc annotate -n openshift-machine-api machines $worker machine.openshift.io/cluster-api-delete-machine=1 --overwrite
      environment:
        PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
        KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
      loop: "{{ range(0, osp_controller_scale, 1)|list }}"

    - name: get current {{ ocp_cluster_name }}-worker-0 replicas
      become_user: ocp
      shell: >
        oc -n openshift-machine-api get machineset {{ ocp_cluster_name }}-worker-0 -o json | jq -r .status.replicas
      environment:
        PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
        KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
      register: worker_replicas

    - name: scale down worker machineset
      become_user: ocp
      shell: >
        oc -n openshift-machine-api scale machineset {{ ocp_cluster_name }}-worker-0 --replicas={{ worker_replicas.stdout|int - osp_controller_scale|int }}
      environment:
        PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
        KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"

    - name: wait until scaled down worker vms are switched off
      become_user: root
      shell: >
        virsh list --state-shutoff | grep "{{ ocp_cluster_name }}_worker_{{ item }}"
      environment:
        PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
        KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
      register: worker_osp_controller_down
      until: worker_osp_controller_down.rc == 0
      delay: 30
      retries: 30
      loop: "{{ range(0, osp_controller_scale, 1)|list }}"

    - name: increase vcpus/memory for worker VM to fit nested controller VM
      become_user: root
      shell: |
        virsh setvcpus  "{{ ocp_cluster_name }}_worker_{{ item }}" 10 --maximum --config
        virsh setvcpus  "{{ ocp_cluster_name }}_worker_{{ item }}" 10 --config
        virsh setmaxmem "{{ ocp_cluster_name }}_worker_{{ item }}" 30G --config
        virsh setmem "{{ ocp_cluster_name }}_worker_{{ item }}" 30G --config
      environment:
        PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
        KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
      loop: "{{ range(0, osp_controller_scale, 1)|list }}"

    - name: scale up worker-osp-controller machineset by {{ osp_controller_scale }}
      become_user: ocp
      shell: >
        oc -n openshift-machine-api scale machineset {{ ocp_cluster_name }}-worker-osp-controller-0 --replicas={{ osp_controller_scale }}
      environment:
        PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
        KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"

    - name: wait until worker-osp-controller got provisioned
      become_user: ocp
      shell: >
        echo $(oc get nodes | grep "worker-{{ item }}" | awk '{ print $2 }')
      environment:
        PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
        KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
      register: worker_osp_controller_created
      until: worker_osp_controller_created.stdout == "Ready"
      delay: 30
      retries: 30
      loop: "{{ range(0, osp_controller_scale , 1)|list }}"

  - name: check if workers are already labeled
    become_user: ocp
    shell: >
      oc get nodes -o=custom-columns="name:.metadata.name,labels:.metadata.labels" | grep vmi
    environment:
      PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
      KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
    register: vmi_labeled
    ignore_errors: true

  - name: label workers to vmi=controller-X
    become_user: ocp
    command: oc label nodes worker-{{ item }} vmi=controller-{{ item }} --overwrite
    loop: "{{ range(0, osp_controller_scale , 1)|list }}"
    environment:
      PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
      KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
    when: vmi_labeled.rc == 1

  - name: check if controller vms are already created
    shell: >
      oc get vmi | grep controller
    environment:
      PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
      KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"
    register: controller_vms_created
    ignore_errors: true

  - name: create working dir for rendered templates
    become_user: ocp
    file:
      path: "{{ base_path }}/cnv"
      state: directory
      mode: '0755'

  - name: create controllers
    when: controller_vms_created.rc == 1
    block:
    - name: create controller-X.yaml from template
      become_user: ocp
      template:
        src:  "controller-X.yaml.j2"
        dest: "{{ base_path }}/cnv/controller-{{ item }}.yaml"
        mode: 0644
      vars:
        osp_controller_mac_address: "{{ '00:14:cd:2b:c8:0%01x' | format(item) }}"
      loop: "{{ range(0, osp_controller_scale , 1)|list }}"

    - name: create comtroller VMs
      become_user: ocp
      command: oc create -f {{ base_path }}/cnv/controller-{{ item }}.yaml
      loop: "{{ range(0, osp_controller_scale , 1)|list }}"
      args:
        chdir: "{{ base_path }}"
      environment:
        PATH: "/usr/local/bin:{{ ansible_env.PATH }}"
        KUBECONFIG: "{{base_path}}/dev-scripts/ocp/{{ ocp_cluster_name }}/auth/kubeconfig"

  - name: get the username running the deploy
    become: false
    local_action: command whoami
    register: username_on_the_host

  - name: Copy {{ converged_ssh_key }} ssh key from ocp, used to test when controller vm is up
    copy:
      src: "{{ converged_ssh_key }}"
      dest: "~{{ username_on_the_host.stdout }}/.ssh/"
      remote_src: yes
      mode: 0600
      owner: "{{ username_on_the_host.stdout }}"

  - name: add controller nodes to inventory
    add_host:
      hostname: "{{ '192.168.25.2%01x' | format(item) }}"
      groups: controller
      ansible_ssh_private_key_file: ~{{ username_on_the_host.stdout }}/.ssh/id_rsa_osp"
      ansible_user: root
    loop: "{{ range(0, osp_controller_scale , 1)|list }}"

- hosts: controller
  gather_facts: no
  tasks:
  - name: Wait for controller vm to come up. Wait for 600 seconds, but only start checking after 60 seconds
    wait_for_connection:
      delay: 60
      timeout: 600
